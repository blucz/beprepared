{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>beprepared is an easy and efficient way to prepare high quality image datasets for diffusion model fine-tuning.</p> <p>It falicates both human and machine-driven data prep work in a non-destructive environment that aggressively  avoids duplicated effort, even as your workflow evolves.</p> <p>It is the most efficient way for one person to prepare image data sets with thousands of images or more.</p>"},{"location":"#code","title":"Code","text":"<p>The code is published on GitHub.</p> <p>Pull requests are welcome.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install beprepared from PyPI:</p> <pre><code>$ pip install beprepared\n</code></pre>"},{"location":"#development-philosophy","title":"Development Philosophy","text":"<p>While beprepared is a technical product, user-experience is the primary focus. By providing a good user experience, people will create better datasets and better models. This is a win for everyone.</p> <p>beprepared is batteries-included software. While it's possible to define nodes in other PyPI packages, the first  choice should be to merge new functionality into <code>blucz/beprepared</code>. Extensions are not on our roadmap. While extension  ecosystems are powerful, they can also lead to confusing user experiences that are not beginner friendly, fragmentation,  and difficulty discovering functionality. </p>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#how-to-run","title":"How to run","text":"<p>To run the examples, save them as Python files (e.g. <code>workflow.py</code>) and run them using:</p> <pre><code>beprepared run workflow.py\n</code></pre> <p>You will almost always use a <code>Load</code> node to import images and a <code>Save</code> node to write out the cleaned data set. What comes in  between can be simple or complex.</p> <p>When you invoke a <code>Human*</code> node like <code>HumanFilter</code> or <code>HumanTag</code>, beprepared will launch a web-based interface on port 8989.  If there are un-filtered or un-tagged images, you will be prompted to go to the web interface and perform filtering.</p> <p>By convention, <code>Save</code> will place output images in the <code>output/</code> directory. For each image, there will be a companion <code>.txt</code> file containing that image's caption. There will also be a <code>.json</code> file which contains all of the image's metadata. Finally, there is an <code>index.html</code> which allows you to view the images and their metadata in a web browser.</p>"},{"location":"examples/#captioning-with-a-trigger-word","title":"Captioning with a trigger word","text":"<p>This is a simple example of how to use beprepared to caption images based on a trigger word.</p> <pre><code>(\n    Load(\"/path/to/photos_of_me\") \n    &gt;&gt; FilterBySize(min_edge=512)   \n    &gt;&gt; ConvertFormat(\"JPEG\")\n    &gt;&gt; Dedupe\n    &gt;&gt; SetCaption(\"ohwx person\")\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#auto-captioning-using-joycaption","title":"Auto-captioning using JoyCaption","text":"<pre><code>(\n    Load(\"/path/to/photos_of_me\")\n    &gt;&gt; FilterBySize(min_edge=512)\n    &gt;&gt; ConvertFormat(\"JPEG\")\n    &gt;&gt; JoyCaptionAlphaOne\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#fuzzy-deduplication","title":"Fuzzy deduplication","text":"<p>This example shows how to use FuzzyDedupe to remove duplicate images based on CLIP embeddings.</p> <pre><code>(\n    Load(\"/path/to/photos_of_me\")\n    &gt;&gt; ClipEmbed\n    &gt;&gt; FuzzyDedupe\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#filtering-based-on-aesthetic-score","title":"Filtering based on Aesthetic Score","text":"<p>This example shows how to use select the best 100 albums based on their aesthetic score.</p> <pre><code>(\n    Load(\"/path/to/photos_of_me\")\n    &gt;&gt; AestheticScore\n    &gt;&gt; Sorted(lambda image: image.aesthetic_score.value, reverse=True)\n    &gt;&gt; Take(100)\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#filtering-out-nsfw-content","title":"Filtering out NSFW content","text":"<p>This example shows how to filter NSFW content using NudeNet</p> <pre><code>(\n    Load(\"/path/to/images\")\n    &gt;&gt; NudeNet\n    &gt;&gt; Filter(lambda image: not image.has_nudity)\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#filter-images-manually-then-caption-with-gpt4o","title":"Filter images manually then caption with GPT4o","text":"<p>To run this example, you will need to set <code>OPENAI_API_KEY</code> in your environment.</p> <pre><code>(\n    Load(\"/path/to/photos_of_me\")\n    &gt;&gt; FilterBySize(min_edge=512)\n    &gt;&gt; ConvertFormat(\"JPEG\")\n    &gt;&gt; HumanFilter\n    &gt;&gt; GPT4oCaption\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#manually-tag-images","title":"Manually tag images","text":"<pre><code>(\n    Load(\"/path/to/photos_of_dogs\")\n    &gt;&gt; FilterBySize(min_edge=512)\n    &gt;&gt; ConvertFormat(\"JPEG\")\n    &gt;&gt; HumanFilter\n    &gt;&gt; HumanTag(tags=[\"labrador\", \"golden retriever\", \"poodle\"])\n    &gt;&gt; Apply(lambda image: image.caption.value = ', '.join(['dog'] + image.tags.value))\n    &gt;&gt; Save\n)\n</code></pre>"},{"location":"examples/#captioning-a-mix-of-sfw-and-nsfw-content-using-various-vlms","title":"Captioning a mix of SFW and NSFW content using various VLMs","text":"<p>Some VLMs are more NSFW-friendly than others. This workflow shows how to split the workflow and use different caption strategies for NSFW content.</p> <pre><code>all = (\n    Load(\"/path/to/images\")\n    &gt;&gt; FilterBySize(min_edge=512)\n    &gt;&gt; ConvertFormat(\"JPEG\")\n    &gt;&gt; NudeNet\n)\n\nwith_nudity    = all &gt;&gt; Filter(lambda image: image.has_nudity) &gt;&gt; JoyCaptionAlphaOne\nwithout_nudity = all &gt;&gt; Filter(lambda image: not image.has_nudity) &gt;&gt; GPT4oCaption\n\nConcat(with_nudity, without_nudity) &gt;&gt; Save\n</code></pre>"},{"location":"examples/#captioning-using-multiple-vlms","title":"Captioning using multiple VLMs","text":"<p>This workflow shows how to use multiple VLMs to caption an image, and then combine the results into a single caption using <code>LLMCaptionTransform</code>.</p> <p>To run this example, you will need to set <code>OPENAI_API_KEY</code> and <code>TOGETHER_AI_API_KEY</code> in your environment. </p> <p>LLM APIs are accessed using litellm, and any model string supported by <code>litellm</code> should work here.</p> <pre><code>(\n    Load(\"/path/to/images\")\n    &gt;&gt; FilterBySize(min_edge=512)\n    &gt;&gt; ConvertFormat(\"JPEG\")\n    &gt;&gt; JoyCaptionAlphaOne(target_prop='joycaption')\n    &gt;&gt; GPT4oCaption(target_prop='gpt4ocaption')\n    &gt;&gt; XGenMMCaption(target_prop='xgenmmcaption')\n    &gt;&gt; QwenVLCaption(target_prop='qwenvlcaption')\n    &gt;&gt; LlamaCaption(target_prop='llamacaption')\n    &gt;&gt; LLMCaptionTransform('together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n                           lambda image: f\"\"\"\n</code></pre> <p>Multiple VLMs have captioned this image. These are their results: </p> <ul> <li>JoyCaption: {image.joycaption.value}</li> <li>GPT4oCaption: {image.gpt4ocaption.value}</li> <li>XGenMMCaption: {image.xgenmmcaption.value}</li> <li>QwenVLCaption: {image.qwenvlcaption.value}</li> <li>LlamaCaption: {image.llamacaption.value}</li> </ul> <p>Please generate a final caption for this image based on the above information. Your response should be the caption, with no extra text or boilerplate.                                \"\"\".strip(),                                target_prop='caption')         &gt;&gt; Save     )</p>"},{"location":"hacking/","title":"Hacking","text":""},{"location":"hacking/#coding-standards","title":"Coding Standards","text":"<p>Write pythonic python, declare types at all API surfaces and use google-style docstrings.</p>"},{"location":"hacking/#creating-new-nodes","title":"Creating new Nodes","text":"<p>This is an example node that is defined correctly:</p> <pre><code>class AddTags(Node):\n    '''Adds tags to all images in a dataset'''\n    def __init__(self, tags: str | List[str]):\n        '''Initializes the AddTags node\n\n        Args:\n            tags (str or List[str]): The tags to add to the images\n        '''\n        super().__init__()\n        if isinstance(tags, str):\n            tags = [tags]\n        self.tags = tags\n\n    def eval(self, dataset: Dataset) -&gt; Dataset:\n        for image in dataset.images:\n            if not image.tags.has_value:\n                image.tags = ConstProperty(set(self.tags))\n                continue\n            else:\n                image.tags = ConstProperty(image.tags.value.union(self.tags))\n        return dataset\n</code></pre> <p>Follow this general pattern and things should be great. Note that <code>eval</code> can also accept <code>datasets: List[Dataset]</code>. This is rare, but required for certain nodes like <code>Concat</code>.</p>"},{"location":"hacking/#how-nodes-are-executed","title":"How nodes are executed","text":"<p>Beprepared nodes are executed sequentially in dependency order. While this may be inefficient in some cases, it's necessary because nodes  often compete for limited resources, whether that's a GPU or a Human who needs to perform an operation. This may change in the future,  but for now, nodes run sequentially.</p> <p>Before <code>eval</code> is invoked, the dataset and images are deep-copied. This gives the code functional semantics while  enabling the code itself to be written in the imperative style that's more typical in python. It is safe to \"flow\" the same dataset into multiple nodes, because each node gets its own copy of the dataset and the images within,  and images and datasets may be freely mutated within <code>eval</code></p>"},{"location":"hacking/#the-web-interface","title":"The Web Interface","text":"<p>At startup, <code>beprepared</code> launches a web interface on <code>0.0.0.0:8989</code>.</p> <p>When a node needs a web interface, it supplies an <code>Applet</code> to <code>workspace.web</code>. This applet \"takes over\" the main area of the web interface.</p> <p>Applets are defined in Python using FastAPI and implemented in Vue3 + Bootstrap on the  client side. If you are hacking on the web stuff, you will either need to <code>npm run build</code> in <code>web/</code> after making changes, or <code>npm run dev</code>  and then set up a <code>.env</code> with <code>VITE_API_URL=http://IP_ADDRESS:8989</code>.</p> <p>By convention, each applet is a vue component in <code>beprepared/web/</code>. On the python side, you can define <code>FastAPI</code> endpoints to serve the applet.</p> <p>See <code>beprepared/nodes/humantag.py</code> for an example of how to define an applet and use it in a node.</p>"},{"location":"nodes/aesthetics/","title":"Aesthetics","text":""},{"location":"nodes/aesthetics/#aestheticscore","title":"AestheticScore","text":"<p>Computes an aesthetic score for each image in the dataset using the simple aesthetics predictor model.</p> <p>This is most commonly combined with <code>Sort</code> and <code>Filter</code> nodes in order to select images based on their aesthetic score.</p>"},{"location":"nodes/aesthetics/#parameters","title":"Parameters","text":"<ul> <li><code>batch_size</code> (default: 256): The number of images to process in parallel.</li> </ul>"},{"location":"nodes/aesthetics/#output-properties","title":"Output properties","text":"<ul> <li><code>image.aesthetic_score</code>: The aesthetic score of the image.</li> </ul>"},{"location":"nodes/aesthetics/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; AestheticScore\n</code></pre>"},{"location":"nodes/anonymize/","title":"Face Anonymization","text":""},{"location":"nodes/anonymize/#anonymize","title":"Anonymize","text":"<p>The <code>Anonymize</code> node detects and anonymizes faces in images using the CenterFace face detection model. This is useful for privacy protection when working with datasets containing people. It can also prevent overfitting if your dataset is unbalanced.</p> <p>It is crucial that when you caption your images for lora training, you mention that faces have been blurred. If not, you are likely to cause the model to forget how to generate faces. You may also want to consider using regularization data alongside this technique.</p>"},{"location":"nodes/anonymize/#methods","title":"Methods","text":"<p>The node supports several anonymization methods:</p> <ul> <li><code>AnonymizeMethod.BLUR</code>: Blurs detected faces (default)</li> <li><code>AnonymizeMethod.SOLID</code>: Covers faces with solid black rectangles</li> <li><code>AnonymizeMethod.MOSAIC</code>: Applies a mosaic/pixelation effect to faces</li> <li><code>AnonymizeMethod.NONE</code>: Detects faces but doesn't modify them (useful for testing)</li> </ul>"},{"location":"nodes/anonymize/#parameters","title":"Parameters","text":"<ul> <li><code>method</code> (default: <code>AnonymizeMethod.BLUR</code>): The anonymization method to use</li> <li><code>threshold</code> (default: <code>0.4</code>): Detection confidence threshold (0.0-1.0)</li> <li><code>mask_scale</code> (default: <code>1.0</code>): Scale factor for face masks to ensure complete coverage</li> <li><code>ellipse</code> (default: <code>True</code>): Use elliptical masks instead of rectangular ones (only for BLUR method)</li> <li><code>mosaic_size</code> (default: <code>20</code>): Size of mosaic blocks when using the MOSAIC method</li> </ul>"},{"location":"nodes/anonymize/#gpu-acceleration","title":"GPU Acceleration","text":"<p>The node automatically uses available CUDA GPUs for faster processing. If multiple GPUs are available, it will distribute the workload across them.</p>"},{"location":"nodes/anonymize/#example","title":"Example","text":"<pre><code># Basic usage with default settings (blur faces)\ndataset &gt;&gt; Anonymize()\n\n# Use solid black rectangles with higher detection threshold\ndataset &gt;&gt; Anonymize(method=AnonymizeMethod.SOLID, threshold=0.5)\n\n# Apply mosaic effect with larger blocks\ndataset &gt;&gt; Anonymize(method=AnonymizeMethod.MOSAIC, mosaic_size=30)\n\n# Increase mask size to cover more of the face area\ndataset &gt;&gt; Anonymize(mask_scale=1.2)\n</code></pre>"},{"location":"nodes/anonymize/#notes","title":"Notes","text":"<ul> <li>Face detection works best on clear, front-facing faces</li> <li>Increasing <code>threshold</code> reduces false positives but may miss some faces</li> <li>Increasing <code>mask_scale</code> helps ensure complete face coverage</li> <li>The node caches results, so re-running with the same parameters is very fast</li> </ul>"},{"location":"nodes/captioning/","title":"Captioning","text":""},{"location":"nodes/captioning/#geminicaption","title":"GeminiCaption","text":"<p>Generates captions for images using Gemini 2.0 Flash Vision.</p> <p>You must set either <code>GEMINI_API_KEY</code> or <code>GOOGLE_API_KEY</code> in your environment to use this node.</p>"},{"location":"nodes/captioning/#parameters","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>prompt</code> (default: <code>None</code>): The prompt to use for the Gemini model</li> <li><code>instructions</code> (default: <code>None</code>): Additional instructions to append to the prompt</li> <li><code>parallel</code> (default: <code>8</code>): The number of images to process in parallel. Adjust based on API rate limits.</li> </ul>"},{"location":"nodes/captioning/#output-properties","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; GeminiCaption()\n</code></pre>"},{"location":"nodes/captioning/#gpt4ocaption","title":"GPT4oCaption","text":"<p>Generates captions for images using GPT4o.</p> <p>You must set <code>OPENAI_API_KEY</code> in your environment to use this node.</p>"},{"location":"nodes/captioning/#parameters_1","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>caption_type</code> (default: <code>'descriptive'</code>): The type of caption to generate (<code>'descriptive'</code> or <code>'booru'</code>)</li> <li><code>prompt</code> (default: <code>None</code>): The prompt to use for the GPT-4o model (read the code if you're curious about customizing this)</li> <li><code>instructions</code> (default: <code>None</code>): Additional instructions to append to the prompt</li> <li><code>parallel</code> (default: <code>8</code>): The number of images to process in parallel. Adjust based on OpenAI rate limits.</li> </ul>"},{"location":"nodes/captioning/#output-properties_1","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_1","title":"Example","text":"<pre><code>dataset &gt;&gt; Gpt4oCaption(caption_type='descriptive')\n</code></pre>"},{"location":"nodes/captioning/#joycaptionalphaone","title":"JoyCaptionAlphaOne","text":"<p>Generates captions for images using JoyCaption Alpha One</p>"},{"location":"nodes/captioning/#parameters_2","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>caption_type</code> (default: <code>'descriptive'</code>): The type of caption to generate (<code>'descriptive'</code>, <code>'stable_diffusion'</code>, or <code>'booru'</code>)</li> <li><code>caption_tone</code> (default: <code>'formal'</code>): The tone of the caption (<code>'formal'</code> or <code>'informal'</code>)</li> <li><code>caption_length</code> (default: <code>'any'</code>): The length of the caption (<code>'any'</code> or an integer)</li> <li><code>batch_size</code> (default: <code>4</code>): The number of images to process in parallel</li> </ul>"},{"location":"nodes/captioning/#output-properties_2","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_2","title":"Example","text":"<pre><code>dataset &gt;&gt; JoyCaptionAlphaOne\n</code></pre>"},{"location":"nodes/captioning/#joycaptionalphatwo","title":"JoyCaptionAlphaTwo","text":"<p>Generates captions for images using JoyCaption Alpha Two with additional caption types and options.</p>"},{"location":"nodes/captioning/#parameters_3","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>caption_type</code> (default: <code>'descriptive'</code>): The type of caption to generate. Options:</li> <li><code>'descriptive'</code></li> <li><code>'descriptive_informal'</code></li> <li><code>'training_prompt'</code></li> <li><code>'midjourney'</code></li> <li><code>'booru_tag_list'</code></li> <li><code>'booru_like_tag_list'</code></li> <li><code>'art_critic'</code></li> <li><code>'product_listing'</code></li> <li><code>'social_media_post'</code></li> <li><code>caption_length</code> (default: <code>'long'</code>): The length of the caption (<code>'any'</code>, <code>'very short'</code>, <code>'short'</code>, <code>'medium-length'</code>, <code>'long'</code>, <code>'very long'</code>, or an integer)</li> <li><code>extra_options</code> (default: <code>[]</code>): List of extra options to include in the caption</li> <li><code>name_input</code> (default: <code>''</code>): Name to use when referring to people/characters in the image</li> <li><code>batch_size</code> (default: <code>4</code>): The number of images to process in parallel</li> </ul>"},{"location":"nodes/captioning/#output-properties_3","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_3","title":"Example","text":"<pre><code>dataset &gt;&gt; JoyCaptionAlphaTwo(\n    caption_type='art_critic',\n    caption_length='very long',\n    extra_options=['Include information about lighting'],\n    name_input='Alice'\n)\n</code></pre>"},{"location":"nodes/captioning/#llamacaption","title":"LlamaCaption","text":"<p>Generates captions for images using <code>meta-llama/llama-3.2-11B-Vision-Instruct</code>.</p> <p>In order to download this model, you need to be logged into huggingface.</p>"},{"location":"nodes/captioning/#parameters_4","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>prompt</code> (default: <code>None</code>): The prompt to use for the Llama model (read the code)</li> <li><code>instructions</code> (default: <code>None</code>): Additional instructions to include in the prompt</li> <li><code>batch_size</code> (default: <code>1</code>): The number of images to process in parallel. If you are running out of memory, try reducing this value.</li> </ul>"},{"location":"nodes/captioning/#output-properties_4","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_4","title":"Example","text":"<pre><code>dataset &gt;&gt; LlamaCaption\n</code></pre>"},{"location":"nodes/captioning/#llmcaptionvariations","title":"LLMCaptionVariations","text":"<p>Generates variations of existing image captions using LLaMA 3.1 8B Instruct. This preserves the original caption and adds numbered variations as new properties.</p>"},{"location":"nodes/captioning/#parameters_5","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): Base property name to store variations in. Will append _1, _2 etc.</li> <li><code>variations</code> (default: <code>2</code>): Number of variations to generate per image</li> <li><code>parallel</code> (default: <code>20</code>): The number of images to process in parallel</li> <li><code>temperature</code> (default: <code>0.7</code>): The temperature to use when sampling from the model</li> <li><code>model</code> (default: <code>'together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo'</code>): The LLM model to use for generating variations</li> </ul>"},{"location":"nodes/captioning/#output-properties_5","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The original caption (preserved)</li> <li><code>image.{target_prop}_1</code>: First variation</li> <li><code>image.{target_prop}_2</code>: Second variation (if variations &gt; 1)</li> <li>etc.</li> </ul>"},{"location":"nodes/captioning/#example_5","title":"Example","text":"<pre><code># Generate 3 variations of each caption\ndataset &gt;&gt; LLMCaptionVariations(variations=3)\n</code></pre>"},{"location":"nodes/captioning/#llmcaptiontransform","title":"LLMCaptionTransform","text":"<p>Computes image captions using a Large Language Model. This can be used for a number of purposes included:</p> <ul> <li>Cleaning up captions from VLMs</li> <li>Computing better captions by combining results from multiple VLMs</li> <li>Computing captions based on tags or other metadata</li> <li>Combining tags or other metadata with VLM caption results</li> <li>Transforming captions into different styles (e.g. fluid vs booru).</li> </ul> <p>And so on. This is commonly one of the last nodes in a workflow, that determines the final caption.</p> <p>Language models are accessed via API using litellm. Model strings  should follow their conventions. For example:</p> <ul> <li><code>together_ai/meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo</code></li> <li><code>together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo</code></li> <li><code>openai/gpt-4o</code></li> <li><code>openai/gpt-4o-mini</code></li> </ul> <p>Depending on the platform used, you will need to set environment variables like <code>OPENAI_API_KEY</code> or <code>TOGETHER_API_KEY</code> or <code>ANTHROPIC_API_KEY</code> to access the models.</p> <p>Be aware that OpenAI and Anthropic models are relatively censored and may refuse certain tasks based on the content. We have had good experiences with Llama and Qwen models.</p> <p>LLM results are cached with the model, prompt, and parameters as the key, beprepared will never execute the same request twice.</p> <p>We plan to support local LLMs in the future, and would welcome pull requests that implement this efficiently. </p>"},{"location":"nodes/captioning/#parameters_6","title":"Parameters","text":"<ul> <li><code>model</code>: The name of the language model to use</li> <li><code>prompt</code>: A function that takes an image and returns a prompt for the language model</li> <li><code>target_prop</code> (default: <code>caption</code>): The property to store the transformed caption in</li> <li><code>parallel</code> (default: <code>20</code>): The number of images to process in parallel. Adjust based on rate limits</li> <li><code>temperature</code> (default: <code>0.5</code>): The temperature to use when sampling from the language model. In general, lower temperatures give more consistent and \"safe\" results and reduce the language model's tendency to hallucinate.</li> </ul>"},{"location":"nodes/captioning/#output-properties_6","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The transformed caption</li> </ul>"},{"location":"nodes/captioning/#example_6","title":"Example","text":"<pre><code>\n            dataset \n            &gt;&gt; JoyCaptionAlphaOne(target_prop='joycaption')\n            &gt;&gt; GPT4oCaption(target_prop='gpt4ocaption')\n            &gt;&gt; XGenMMCaption(target_prop='xgenmmcaption')\n            &gt;&gt; QwenVLCaption(target_prop='qwenvlcaption')\n            &gt;&gt; LlamaCaption(target_prop='llamacaption')\n            &gt;&gt; Gemma3Caption(target_prop='gemma3caption')\n            &gt;&gt; LLMCaptionTransform('together_ai/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo',\n                                   lambda image: f\"\"\"\n    Multiple VLMs have captioned this image. These are their results: \n\n    - JoyCaption: {image.joycaption.value}\n    - GPT4oCaption: {image.gpt4ocaption.value}\n    - XGenMMCaption: {image.xgenmmcaption.value}\n    - QwenVLCaption: {image.qwenvlcaption.value}\n    - LlamaCaption: {image.llamacaption.value}\n    - Gemma3: {image.gemma3caption.value}\n\n    Please generate a final caption for this image based on the above information. Your response should be the caption, with no extra text or boilerplate.\n                                   \"\"\".strip(),\n                                   target_prop='caption')\n</code></pre>"},{"location":"nodes/captioning/#qwenvlcaption","title":"QwenVLCaption","text":"<p>Generates captions for images using <code>Qwen/Qwen2-VL-7B-Instruct</code>.</p>"},{"location":"nodes/captioning/#parameters_7","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>prompt</code> (default: <code>None</code>): The prompt to use for the Qwen 2 VL 7B model (read the code)</li> <li><code>instructions</code> (default: <code>None</code>): Additional instructions to include in the prompt</li> <li><code>batch_size</code> (default: <code>1</code>): The number of images to process in parallel. If you are running out of memory, try reducing this value.</li> </ul>"},{"location":"nodes/captioning/#output-properties_7","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_7","title":"Example","text":"<pre><code>dataset &gt;&gt; QwenVLCaption\n</code></pre>"},{"location":"nodes/captioning/#gemma3caption","title":"Gemma3Caption","text":"<p>Generates captions for images using Google's Gemma 3 12B Instruction-Tuned Vision-Language Model.</p> <p>Gemma 3 is a powerful multimodal model that can process both text and images, generating high-quality text outputs. It supports a 128K token context window and is multilingual, supporting over 140 languages. The model excels at detailed image descriptions, visual question answering, and image analysis.</p> <p>Requirements: - Transformers version: Requires <code>transformers &gt;= 4.46.0</code>. Run <code>pip install --upgrade transformers</code> if you encounter model loading errors. - VRAM: Gemma 3 12B requires significant VRAM (24GB+ recommended). Consider using batch_size=1 to manage memory usage.</p>"},{"location":"nodes/captioning/#parameters_8","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>prompt</code> (default: Detailed description prompt): The prompt to use for image captioning</li> <li><code>system_prompt</code> (default: <code>None</code>): System prompt to set the assistant's behavior</li> <li><code>instructions</code> (default: <code>None</code>): Additional instructions to append to the prompt</li> <li><code>batch_size</code> (default: <code>1</code>): The number of images to process in parallel. Keep at 1 for 12B model.</li> </ul>"},{"location":"nodes/captioning/#output-properties_8","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_8","title":"Example","text":"<pre><code># Basic usage with default detailed captioning\ndataset &gt;&gt; Gemma3Caption()\n\n# Custom prompt for specific focus\ndataset &gt;&gt; Gemma3Caption(\n    prompt='What objects are visible in this image?'\n)\n\n# With system prompt and instructions\ndataset &gt;&gt; Gemma3Caption(\n    system_prompt='You are an art critic analyzing paintings.',\n    prompt='Analyze this artwork',\n    instructions='Focus on style, technique, and emotional impact'\n)\n</code></pre>"},{"location":"nodes/captioning/#passthrough","title":"Passthrough","text":"<p>A node that does nothing and returns the dataset unchanged. This can be useful as a no-op placeholder in conditional pipeline branches.</p>"},{"location":"nodes/captioning/#example_9","title":"Example","text":"<pre><code># Use Passthrough as a no-op alternative in a conditional\ndataset &gt;&gt; (ProcessNode() if condition else Passthrough())\n</code></pre>"},{"location":"nodes/captioning/#mapcaption","title":"MapCaption","text":"<p>Maps the current caption to a new caption using a function.</p>"},{"location":"nodes/captioning/#parameters_9","title":"Parameters","text":"<ul> <li><code>func</code>: Function that takes the current caption string and returns a new caption string.</li> </ul>"},{"location":"nodes/captioning/#output-properties_9","title":"Output Properties","text":"<ul> <li><code>image.caption</code>: The transformed caption.</li> </ul>"},{"location":"nodes/captioning/#example_10","title":"Example","text":"<pre><code># Add an exclamation mark to all captions\ndataset &gt;&gt; MapCaption(lambda caption: caption + \"!\")\n\n# Prepend a prefix to all captions\ndataset &gt;&gt; MapCaption(lambda caption: f\"A photo showing {caption}\")\n</code></pre>"},{"location":"nodes/captioning/#setcaption","title":"SetCaption","text":"<p>Sets the <code>caption</code> property on an image.</p>"},{"location":"nodes/captioning/#parameters_10","title":"Parameters","text":"<ul> <li><code>caption</code>: The caption to set.</li> </ul>"},{"location":"nodes/captioning/#output-properties_10","title":"Output Properties","text":"<ul> <li><code>image.caption</code>: The caption of the image.</li> </ul>"},{"location":"nodes/captioning/#example_11","title":"Example","text":"<pre><code>dataset &gt;&gt; SetCaption(\"ohwx person\")\n</code></pre>"},{"location":"nodes/captioning/#xgenmmcaption","title":"XGenMMCaption","text":"<p>Generates captions for images using <code>Salesforce/xgen-mm-phi3-mini-instruct-r-v1</code>.</p>"},{"location":"nodes/captioning/#parameters_11","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>prompt</code> (default: <code>None</code>): The prompt to use for the xGen-mm model (read the code)</li> <li><code>instructions</code> (default: <code>None</code>): Additional instructions to include in the prompt</li> <li><code>batch_size</code> (default: <code>4</code>): The number of images to process in parallel. If you are running out of memory, try reducing this value.</li> </ul>"},{"location":"nodes/captioning/#output-properties_11","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_12","title":"Example","text":"<pre><code>dataset &gt;&gt; XGenMMCaption\n</code></pre>"},{"location":"nodes/captioning/#florence2caption","title":"Florence2Caption","text":"<p>Generates captions for images using the Florence-2-large model.</p>"},{"location":"nodes/captioning/#parameters_12","title":"Parameters","text":"<ul> <li><code>target_prop</code> (default: <code>'caption'</code>): The property to store the caption in</li> <li><code>task</code> (default: <code>Florence2Task.MORE_DETAILED_CAPTION</code>): The captioning task to perform. One of:</li> <li><code>Florence2Task.CAPTION</code>: Basic caption</li> <li><code>Florence2Task.DETAILED_CAPTION</code>: Detailed caption</li> <li><code>Florence2Task.MORE_DETAILED_CAPTION</code>: More detailed caption</li> <li><code>batch_size</code> (default: <code>8</code>): The number of images to process in parallel. If you are running out of memory, try reducing this value.</li> </ul>"},{"location":"nodes/captioning/#output-properties_12","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The caption generated for the image</li> </ul>"},{"location":"nodes/captioning/#example_13","title":"Example","text":"<pre><code>dataset &gt;&gt; Florence2Caption(task=Florence2Task.DETAILED_CAPTION)\n</code></pre>"},{"location":"nodes/clip/","title":"CLIP","text":""},{"location":"nodes/clip/#clipembed","title":"ClipEmbed","text":"<p>Computes a CLIP embedding for each image in the dataset using the <code>openai/clip-vit-large-patch14</code> model.</p> <p>CLIP embedding is quite fast and well worth it, as it unlocks a lot of functionality.</p> <p>This node computes CLIP embeddings efficiently in batch, and supports multi-GPU operation. Several other  nodes depend on CLIP embeddings, for example <code>FuzzyDedupe</code> and <code>SmartHumanFilter</code>. </p> <p>You may also use beprepared to compute CLIP embeddings for your own purposes, which can be consumed out  of the <code>image.json</code> sidecars in the output directory.</p>"},{"location":"nodes/clip/#parameters","title":"Parameters","text":"<ul> <li><code>batch_size</code> (default: 128): The number of images to process in parallel.</li> <li><code>target_property</code> (default: \"clip\"): The property to store the CLIP embedding in.</li> </ul>"},{"location":"nodes/clip/#output-properties","title":"Output properties","text":"<ul> <li><code>image.{target_property}</code>: The CLIP embedding for the image.</li> </ul>"},{"location":"nodes/clip/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; ClipEmbed\n</code></pre>"},{"location":"nodes/convertformat/","title":"Format Conversion","text":""},{"location":"nodes/convertformat/#convertformat","title":"ConvertFormat","text":"<p>Converts images to a specified format. Images already in that format will not be modified. </p> <p>The most common use cases are converting to <code>PNG</code> or <code>JPEG</code> for compatibility with training pipelines  that do not support <code>WEBP</code> or other formats.</p>"},{"location":"nodes/convertformat/#parameters","title":"Parameters","text":"<ul> <li><code>format</code>: The format to convert to as a PIL format string, e.g. <code>'JPEG'</code>, <code>'PNG'</code>, <code>'WEBP'</code></li> </ul>"},{"location":"nodes/convertformat/#output-properties","title":"Output properties","text":"<ul> <li><code>image.format</code>: The format of the image, after conversion, e.g. <code>'PNG'</code></li> </ul>"},{"location":"nodes/convertformat/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; ConvertFormat('PNG')\n</code></pre>"},{"location":"nodes/crop/","title":"Cropping Nodes","text":""},{"location":"nodes/crop/#croptoaspect","title":"CropToAspect","text":"<p>The <code>CropToAspect</code> node intelligently crops images to match specified aspect ratios. This is particularly useful for preparing datasets that need consistent aspect ratios for training or when targeting specific output formats.</p>"},{"location":"nodes/crop/#features","title":"Features","text":"<ul> <li>Smart Aspect Matching: Automatically selects the closest aspect ratio from your list to minimize cropping</li> <li>Center Cropping: Preserves the most important content by cropping from the center</li> <li>Efficient Caching: Processes each image only once, even across multiple workflow runs</li> </ul>"},{"location":"nodes/crop/#parameters","title":"Parameters","text":"<ul> <li><code>aspect_ratios</code> (List[float]): List of target aspect ratios (width/height)</li> <li>Example: <code>[0.5, 1.0, 1.5]</code> for portrait, square, and landscape</li> <li>Example: <code>[1.0, 1.91, 0.8]</code> for common social media formats</li> </ul>"},{"location":"nodes/crop/#how-it-works","title":"How It Works","text":"<ol> <li>For each image, the node calculates its original aspect ratio</li> <li>Finds the closest matching ratio from your provided list</li> <li>Calculates center crop dimensions to achieve the target ratio</li> <li>Crops and saves the image with the new aspect ratio</li> </ol>"},{"location":"nodes/crop/#usage-examples","title":"Usage Examples","text":""},{"location":"nodes/crop/#basic-usage","title":"Basic Usage","text":"<pre><code>from beprepared import *\n\n(\n    Load(\"raw_images\")\n    &gt;&gt; CropToAspect(aspect_ratios=[1.0])  # Crop all to squares\n    &gt;&gt; Save(\"square_images\")\n)\n</code></pre>"},{"location":"nodes/crop/#multiple-aspect-ratios","title":"Multiple Aspect Ratios","text":"<pre><code># Prepare dataset with common aspect ratios\n(\n    Load(\"diverse_images\")\n    &gt;&gt; CropToAspect(\n        aspect_ratios=[0.75, 1.0, 1.33]  # Portrait, square, landscape\n    )\n    &gt;&gt; Save(\"normalized_aspects\")\n)\n</code></pre>"},{"location":"nodes/crop/#social-media-formats","title":"Social Media Formats","text":"<pre><code># Crop for various social media platforms\n(\n    Load(\"content\")\n    &gt;&gt; CropToAspect(\n        aspect_ratios=[\n            1.0,    # Instagram square\n            0.8,    # Instagram portrait (4:5)\n            1.91,   # Instagram landscape (1.91:1)\n            0.5625, # Instagram story (9:16)\n        ]\n    )\n    &gt;&gt; Save(\"social_ready\")\n)\n</code></pre>"},{"location":"nodes/crop/#combined-with-size-filtering","title":"Combined with Size Filtering","text":"<pre><code># Full preprocessing pipeline\n(\n    Load(\"raw_dataset\")\n    &gt;&gt; FilterBySize(min_edge=1024)         # Remove small images first\n    &gt;&gt; CropToAspect(\n        aspect_ratios=[0.67, 1.0, 1.5]    # Common training ratios\n    )\n    &gt;&gt; FilterBySize(min_edge=512)          # Filter based on cropped size\n    &gt;&gt; JoyCaptionAlphaOne                  # Caption the cropped images\n    &gt;&gt; Save(\"training_ready\")\n)\n</code></pre>"},{"location":"nodes/crop/#properties-added","title":"Properties Added","text":"<p>The node adds several properties to track the cropping operation:</p> <ul> <li><code>crop_target_ratio</code>: The aspect ratio that was applied</li> <li><code>crop_original_width</code>: Original image width before cropping</li> <li><code>crop_original_height</code>: Original image height before cropping</li> </ul> <p>These can be useful for debugging or further processing:</p> <pre><code>(\n    Load(\"images\")\n    &gt;&gt; CropToAspect(aspect_ratios=[1.0, 1.5])\n    &gt;&gt; Filter(lambda img: img.crop_target_ratio.value == 1.0)  # Only squares\n    &gt;&gt; Save(\"squares_only\")\n)\n</code></pre>"},{"location":"nodes/crop/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Images are cached after processing, so re-running workflows is fast</li> <li>The node loads images one at a time to minimize memory usage</li> <li>Consider filtering very small images first with <code>FilterBySize</code> to avoid unnecessary processing</li> </ul>"},{"location":"nodes/crop/#common-use-cases","title":"Common Use Cases","text":"<ol> <li>Training Set Preparation: Normalize aspect ratios for consistent batch processing</li> <li>Multi-Aspect Training: Prepare datasets with specific aspect ratio buckets</li> <li>Platform-Specific Content: Crop images for specific social media or display requirements</li> <li>Aspect Ratio Standardization: Ensure all images match specific aspect ratios</li> </ol>"},{"location":"nodes/crop/#tips","title":"Tips","text":"<ul> <li>Order aspect ratios from most to least common in your dataset for better matching</li> <li>Use <code>FilterBySize</code> before <code>CropToAspect</code> to pre-filter obviously unsuitable images</li> <li>Use <code>FilterBySize</code> after <code>CropToAspect</code> if you need to ensure minimum dimensions</li> <li>For maximum flexibility, include a wide range of aspect ratios</li> <li>The node always preserves the center of the image, so ensure important content is centrally located</li> </ul>"},{"location":"nodes/dedupe/","title":"Deduplication","text":""},{"location":"nodes/dedupe/#exactdedupe","title":"ExactDedupe","text":"<p>This deduplicates images based on their SHA256 hash. This is extremely fast and simple, but does not catch perceptually similar images. </p> <p>If that is a priority, try <code>FuzzyDedupe</code> instead, or better yet use <code>ExactDedupe &gt;&gt; FuzzyDedupe</code> in sequence.</p>"},{"location":"nodes/dedupe/#example","title":"Example","text":"<pre><code>ExactDedupe\n</code></pre>"},{"location":"nodes/dedupe/#fuzzydedupe","title":"FuzzyDedupe","text":"<p>Deduplicates images based on perceptual similarity. </p> <p>This process uses clip embeddings and an ANN (approximate nearest neighbors) index to find groups of images that are within <code>threshold</code> cosine  similarity of each other. You can monitor the clusters by setting <code>debug_html</code> to a path where an HTML file will be saved that shows the images  in each cluster. Using this, you can tune the <code>threshold</code> parameter to get the desired deduplication results.</p> <p>The n_trees and n_neighbors parameters control the accuracy and speed of the ANN index. Higher values will be more accurate but slower. The  default values are good for most cases.</p>"},{"location":"nodes/dedupe/#parameters","title":"Parameters","text":"<ul> <li><code>threshold</code> (default: 0.95): The cosine similarity threshold for images to be considered duplicates.</li> <li><code>debug_html</code> (default: 'fuzzy_dedupe.html'): If set, an HTML file will be saved with the images in each cluster for quality monitoring.</li> <li><code>n_trees</code> (default: 10): The number of trees to build in the ANN index.</li> <li><code>n_neighbors</code> (default: 50): The number of neighbors to search for in the ANN index.</li> </ul>"},{"location":"nodes/dedupe/#example_1","title":"Example","text":"<pre><code># Basic usage showing Clip Encoder and Fuzzy Dedupe\ndataset &gt;&gt; ClipEncode &gt;&gt; FuzzyDedupe\n\n# Advanced usage for tuning parameters\ndataset &gt;&gt; ClipEncode &gt;&gt; FuzzyDedupe(threshold=0.9, debug_html='fuzzy_dedupe.html')\n\n# Adjusting ANN parameters\ndataset &gt;&gt; ClipEncode &gt;&gt; FuzzyDedupe(n_trees=20, n_neighbors=100)\n</code></pre>"},{"location":"nodes/filtering/","title":"Filtering","text":""},{"location":"nodes/filtering/#filter","title":"Filter","text":"<p>The filter node is used to filter images based on a predicate.</p>"},{"location":"nodes/filtering/#parameters","title":"Parameters","text":"<ul> <li><code>predicate</code>: A function that takes an image and returns a boolean.</li> </ul>"},{"location":"nodes/filtering/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; Filter(lambda image: image.aesthetic_score.value &gt; 0.5)\n</code></pre>"},{"location":"nodes/filtering/#filterbyaspectratio","title":"FilterByAspectRatio","text":"<p>Filters images based on their aspect ratio (width/height).</p>"},{"location":"nodes/filtering/#parameters_1","title":"Parameters","text":"<ul> <li><code>min_aspect</code> (default: <code>None</code>): The minimum aspect ratio to keep</li> <li><code>max_aspect</code> (default: <code>None</code>): The maximum aspect ratio to keep</li> </ul>"},{"location":"nodes/filtering/#example_1","title":"Example","text":"<pre><code># Keep only images with aspect ratios between 0.5 and 2.0\ndataset &gt;&gt; FilterByAspectRatio(min_aspect=0.5, max_aspect=2.0)\n\n# Keep only landscape images (wider than tall)\ndataset &gt;&gt; FilterByAspectRatio(min_aspect=1.0)\n\n# Keep only portrait images (taller than wide)\ndataset &gt;&gt; FilterByAspectRatio(max_aspect=1.0)\n</code></pre>"},{"location":"nodes/filtering/#filterbysize","title":"FilterBySize","text":"<p>Excludes images from the dataset based on their size.</p>"},{"location":"nodes/filtering/#parameters_2","title":"Parameters","text":"<ul> <li><code>min_width</code> (default: <code>None</code>): The minimum width of the image</li> <li><code>min_height</code> (default: <code>None</code>): The minimum height of the image</li> <li><code>min_edge</code> (default: <code>None</code>): The minimum edge length of the image</li> <li><code>max_width</code> (default: <code>None</code>): The maximum width of the image</li> <li><code>max_height</code> (default: <code>None</code>): The maximum height of the image</li> <li><code>max_edge</code> (default: <code>None</code>): The maximum edge length of the image</li> </ul>"},{"location":"nodes/filtering/#example_2","title":"Example","text":"<pre><code>dataset &gt;&gt; FilterBySize(min_width=768)\n</code></pre>"},{"location":"nodes/filtering/#humanfilter","title":"HumanFilter","text":"<p>Displays an efficient web interface that a human can use to filter images. Once you enter the <code>HumanFilter</code> step you will be prompted to open a web browser and view the web interface.</p> <p>Filter results are cached, <code>HumanFilter</code> only presents un-filtered images. If all images have been filtered, the web interface is skipped.</p> <p>The web interface is designed to be comfortable on both desktop and mobile platforms. If you have a keyboard, you can efficiently move through the list of image using left/right arrow keys, accept an image using the up arrow, and reject using the down arrow. You can also use WASD keys in the same way if you prefer to filter images left-handed.</p> <p>Filter results are cached in a <code>domain</code>. This allows you to have unrelated filter judgements that are built and maintained separately.</p>"},{"location":"nodes/filtering/#parameters_3","title":"Parameters","text":"<ul> <li><code>domain</code> (default: <code>\"default\"</code>): The domain to use for caching the filter results.</li> </ul>"},{"location":"nodes/filtering/#output-properties","title":"Output properties","text":"<ul> <li><code>image.passed_human_filter</code>: The filter result of the image.</li> </ul>"},{"location":"nodes/filtering/#example_3","title":"Example","text":"<pre><code>dataset &gt;&gt; HumanFilter\n</code></pre>"},{"location":"nodes/load/","title":"Loading Images","text":""},{"location":"nodes/load/#load","title":"Load","text":"<p>Walks a directory of images recursively, loading them into the workspace.</p> <p>During the load process, images are validated and attributes are extracted. Images are hardlinked into <code>_beprepared/objects</code> to track their contents properly while avoiding taking up additional disk space.</p> <p>beprepared currently assumes that images are immutable. This is something we plan to work on, but for now, if you edit an image after processing in beprepared, consider changing the filename before re-processing that dataset so that beprepared picks up the changes.</p>"},{"location":"nodes/load/#parameters","title":"Parameters","text":"<ul> <li><code>dir</code>: The directory to walk for images</li> </ul>"},{"location":"nodes/load/#output-properties","title":"Output properties","text":"<ul> <li><code>image.original_path</code>: The original path of the image</li> <li><code>image.objectid</code>: The sha256 hash of the image</li> <li><code>image.ext</code>: The extension of the image</li> <li><code>image.width</code>: The width of the image</li> <li><code>image.height</code>: The height of the image</li> <li><code>image.format</code>: The format of the image, as determined by the <code>PIL</code> library (e.g. <code>JPEG</code>, <code>PNG</code>, <code>WEBP</code>, etc.)</li> </ul>"},{"location":"nodes/load/#example","title":"Example","text":"<pre><code>Load(.) &gt;&gt; ... other nodes ...\n</code></pre>"},{"location":"nodes/nsfw/","title":"NSFW","text":""},{"location":"nodes/nsfw/#nudenet","title":"NudeNet","text":"<p>NudeNet uses a fine-tuned yolov8 model to detect concepts related to SFW/NSFW distinctions in images.</p>"},{"location":"nodes/nsfw/#parameters","title":"Parameters","text":"<ul> <li><code>threshold</code> (default: 0.5): The minimum confidence required to recognize a concept in the image.</li> </ul>"},{"location":"nodes/nsfw/#output-properties","title":"Output properties","text":"<ul> <li><code>image.has_nudity</code>: Whether the image contains nudity or not.</li> <li><code>image.nudenet</code>: The raw output of the NudeNet model, of type <code>NudeNetDetections</code></li> <li><code>image.nudenet.value.detections</code>: A list of dicts like <code>{ \"class\": \"&lt;label&gt;\", \"score\": 0.8 }</code> which contain the detected concepts and their confidence scores.</li> <li><code>image.nudenet.value.has_nudity</code>: Whether the image contains nudity or not.</li> <li><code>image.nudenet.value.has_female</code>: Whether the image contains a female subject.</li> <li><code>image.nudenet.value.has_male</code>: Whether the image contains a male subject.</li> <li><code>image.nudenet.value.has(label)</code>: Whether the score for a specified label exceeds the threshold.</li> </ul>"},{"location":"nodes/nsfw/#supported-labels","title":"Supported Labels","text":"<pre><code>FEMALE_GENITALIA_COVERED\nFACE_FEMALE\nBUTTOCKS_EXPOSED\nFEMALE_BREAST_EXPOSED\nFEMALE_GENITALIA_EXPOSED\nMALE_BREAST_EXPOSED\nANUS_EXPOSED\nFEET_EXPOSED\nBELLY_COVERED\nFEET_COVERED\nARMPITS_COVERED\nARMPITS_EXPOSED\nFACE_MALE\nBELLY_EXPOSED\nMALE_GENITALIA_EXPOSED\nANUS_COVERED\nFEMALE_BREAST_COVERED\nBUTTOCKS_COVERED\n</code></pre>"},{"location":"nodes/nsfw/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; NudeNet\n</code></pre>"},{"location":"nodes/save/","title":"Saving Datasets","text":""},{"location":"nodes/save/#save","title":"Save","text":"<p>The <code>Save</code> node saves images and captions to a directory, following <code>foo.jpg</code>, <code>foo.txt</code>, <code>bar.jpg</code>, <code>bar.txt</code>, etc. </p> <p>Additionally, an <code>index.html</code> file is generated that can be used to browse the images and their properties for evaluation purposes.</p> <p>Images are hardlinked to the <code>_beprepared/objects</code> directory, so while new files are created, they should not take up additional space on the disk.</p>"},{"location":"nodes/save/#parameters","title":"Parameters","text":"<ul> <li><code>dir</code> (default: <code>output</code>): The directory to save the images to. Relative paths are computed relative to the workspace directory.</li> <li><code>captions</code> (default: <code>True</code>): Whether to save captions files next to image files</li> <li><code>sidecars</code> (default: <code>True</code>): Whether to save .json files next to the image which contain image properties</li> <li><code>caption_ext</code> (default: <code>.txt</code>): The extension to use for captions files</li> </ul>"},{"location":"nodes/save/#example","title":"Example","text":"<pre><code>dataset &gt;&gt; Save\n</code></pre>"},{"location":"nodes/scaling/","title":"Scaling","text":""},{"location":"nodes/scaling/#downscale","title":"Downscale","text":"<p>Downscale images based on edge length constraints. The node supports two mutually exclusive modes: - <code>max_edge</code>: Downscales images only if their largest edge exceeds the specified value - <code>min_edge</code>: Downscales images so their smallest edge equals exactly the specified value</p> <p>Images that don't meet the criteria for downscaling are left unchanged.</p> <p><code>DownscaleMethod.PIL</code> uses the <code>LANCZOS</code> resampling filter.</p>"},{"location":"nodes/scaling/#parameters","title":"Parameters","text":"<ul> <li><code>method</code> (default=<code>DownscaleMethod.PIL</code>): The method to use for downscaling (currently only <code>DownscaleMethod.PIL</code> is supported)</li> <li><code>max_edge</code> (optional): The maximum edge length - scales down only if image's largest edge is larger</li> <li><code>min_edge</code> (optional): The minimum edge length - scales down to make smallest edge exactly this size</li> <li><code>format</code> (default=<code>'PNG'</code>): The format to save the downscaled images in (e.g., <code>'PNG'</code>, <code>'JPEG'</code>, <code>'WEBP'</code>)</li> </ul> <p>Note: You must specify exactly one of <code>max_edge</code> or <code>min_edge</code>. They are mutually exclusive.</p>"},{"location":"nodes/scaling/#output-properties","title":"Output properties","text":"<ul> <li><code>image.width</code>: The width of the image after downscaling</li> <li><code>image.height</code>: The height of the image after downscaling</li> <li><code>image.format</code>: The format of the image after downscaling</li> <li><code>image.objectid</code>: The object ID of the image after downscaling</li> <li><code>image.downscale_info</code>: Dictionary containing scaling information:</li> <li><code>method</code>: The scaling method used</li> <li><code>max_edge</code> or <code>min_edge</code>: The edge constraint value</li> <li><code>original_width</code>: Width before scaling</li> <li><code>original_height</code>: Height before scaling</li> <li><code>scaled_width</code>: Width after scaling</li> <li><code>scaled_height</code>: Height after scaling</li> </ul>"},{"location":"nodes/scaling/#examples","title":"Examples","text":"<pre><code># Downscale images so that the max edge of any image is 1024px\ndataset &gt;&gt; Downscale(max_edge=1024)\n\n# Downscale images so that the minimum edge is exactly 512px\ndataset &gt;&gt; Downscale(min_edge=512)\n\n# Downscale to minimum edge of 768px and save as JPEG\ndataset &gt;&gt; Downscale(min_edge=768, format='JPEG')\n</code></pre>"},{"location":"nodes/scaling/#behavior-details","title":"Behavior Details","text":"<p>With <code>max_edge</code>: - If the largest dimension is already \u2264 max_edge, the image is unchanged - Otherwise, scales down proportionally so the largest dimension equals max_edge</p> <p>With <code>min_edge</code>: - If the smallest dimension is already \u2264 min_edge, the image is unchanged - Otherwise, scales down proportionally so the smallest dimension equals min_edge - Useful for ensuring consistent minimum resolutions after cropping</p>"},{"location":"nodes/scaling/#upscale","title":"Upscale","text":"<p>Upsample images where their longest edge is less than <code>min_edge</code>. Images that are already large enough are not modified.</p> <p><code>UpscaleMethod.PIL</code> uses the <code>LANCZOS</code> resampling filter. <code>UpscaleMethod.SWINIR</code> uses the SwinIR neural network model for high-quality upscaling.</p> <p>NOTE: ESRGAN is partially implemented, but it requires hacks to work because of bugs in <code>basicsr</code>. There is a plan to work around this on our side, but it is not done yet.</p>"},{"location":"nodes/scaling/#parameters_1","title":"Parameters","text":"<ul> <li><code>method</code> (default=<code>UpscaleMethod.PIL</code>): The method to use for upscaling:</li> <li><code>UpscaleMethod.PIL</code>: Fast CPU-based upscaling using LANCZOS filter</li> <li><code>UpscaleMethod.SWINIR</code>: High-quality GPU-based upscaling using SwinIR neural network</li> <li><code>min_edge</code> (default=<code>1024</code>): The minimum edge length for the upscaling</li> <li><code>format</code> (default=<code>'PNG'</code>): The format to save the upscaled images in (e.g., <code>'PNG'</code>, <code>'JPEG'</code>, <code>'WEBP'</code>)</li> </ul>"},{"location":"nodes/scaling/#output-properties_1","title":"Output properties","text":"<ul> <li><code>image.width</code>: The width of the image after upscaling</li> <li><code>image.height</code>: The height of the image after upscaling</li> <li><code>image.format</code>: The format of the image after upscaling</li> <li><code>image.objectid</code>: The object ID of the image after upscaling</li> </ul>"},{"location":"nodes/scaling/#example","title":"Example","text":"<pre><code># Upscale images so that the min edge of any image is 1024px\ndataset &gt;&gt; Upscale(min_edge=1024)\n</code></pre>"},{"location":"nodes/tags/","title":"Tagging","text":"<p>Tags are a way to label images with metadata. They can be used for a number of different purposes:</p> <ul> <li>To track images through the pipeline </li> <li>To filter images based on their tags</li> <li>To capture human tagging effort, and use it to generate captions using <code>LLMCaption</code></li> <li>To capture human tagging effort, and consume it in another tool from the <code>image.json</code> sidecars. </li> </ul>"},{"location":"nodes/tags/#addtags","title":"AddTags","text":"<p>Adds tags to all images in a dataset</p>"},{"location":"nodes/tags/#parameters","title":"Parameters","text":"<ul> <li><code>tags</code>: A list of tags to add to each image</li> </ul>"},{"location":"nodes/tags/#output-properties","title":"Output properties","text":"<ul> <li><code>image.tags</code>: The tags of the image, after adding the new tags</li> </ul>"},{"location":"nodes/tags/#example","title":"Example","text":"<pre><code># Single tag\ndataset &gt;&gt; AddTags(\"labrador\")\n\n# Multiple tags\ndataset &gt;&gt; AddTags([\"labrador\", \"poodle\", \"labradoodle\"])\n</code></pre> <pre><code>    '''HumanTag is a node that allows you to tag images using a web interface.\n\n       The domain is used to separate different sets of tags. For example, you could have a domain for tags \n       related to style and a domain for tags related to content. If you want to keep them separate, use \n       different domains. Likewise, if you have multiple sets of images that use different tagging practices, \n       you will want to use different domains.\n\n       If the tag set evolves and you want to evaluate images again without losing work, increment the \n       version number. This will cause the web interface to re-evaluate all images.\n    '''\n    def __init__(self, \n                 domain: str = 'default', \n                 filter_domain: str = 'default', \n                 version = 1, \n                 tags: List[str] | List[List[str]] = [],\n                 target_prop: str = 'tags',\n                 skip_ui=False):\n</code></pre>"},{"location":"nodes/tags/#humantag","title":"HumanTag","text":"<p>Displays an efficient web interface that a human can use to tag images. Once you enter the <code>HumanTag</code> step you will be prompted to open a web browser and view the web interface.</p> <p>Tagging effort is cached, and <code>HumanTag</code> only presents un-tagged images. If all images have been tagged, the web interface is skipped.</p> <p>Tags are cached in a <code>domain</code>. This allows you to have unrelated sets of tags that are built and maintained separately, which can be useful for multi-concept tagging or merging different datasets or subsets of your dataset differently. In most cases the <code>default</code> domain is sufficient.</p> <p>By default, the Web UI only presents untagged images. If you want to re-present all images for tagging, you can increment <code>version</code> and it will re-present all images.</p> <p>In the HumanTag UI, you can filter images by tapping the trash icon or pressing the down-arrow key. This is useful if you encounter into an undesirable image during tagging, or if you want to tag+filter in one step to avoid visiting each image twice.</p>"},{"location":"nodes/tags/#parameters_1","title":"Parameters","text":"<ul> <li><code>domain</code> (default: <code>\"default\"</code>): The domain to use for the tags. </li> <li><code>version</code> (default: <code>1</code>): The version of the tags</li> <li><code>tags</code> (default: <code>[]</code>): A list of tags to add to each image. This can be a flat list of strings, or a list of lists of strings. In the second case, each list is organized on a new row in the UI.</li> <li><code>target_prop</code> (default: <code>tags</code>): The property name to store the tags in</li> <li><code>skip_ui</code> (default: <code>False</code>): If <code>True</code>, the UI will not be displayed, and some images may be untagged. This is useful as a temporary measure when you want to debug further steps on a dataset without tagging all images.</li> </ul>"},{"location":"nodes/tags/#output-properties_1","title":"Output properties","text":"<ul> <li><code>image.{target_prop}</code>: The tags of the image, after adding the new tags</li> </ul>"},{"location":"nodes/tags/#example_1","title":"Example","text":"<pre><code># Flat list of tags\ndataset &gt;&gt; HumanTag(tags=[\"labrador\", \"poodle\", \"labradoodle\"])\n\n# List of tags with layout for the UI\ndataset &gt;&gt; HumanTag(tags=[\n    [\"Greyhound\", \"Whippet\", \"Afghan Hound\"],                             # Sighthounds\n    [\"Rottweiler\", \"Siberian Husky\", \"Saint Bernard\"],                    # Working Dogs\n    [\"Jack Russell Terrier\", \"Scottish Terrier\", \"Bull Terrier\"]          # Terriers\n    [\"Labrador Retriever\", \"Golden Retriever\", \"Flat-Coated Retriever\"])  # Retrievers\n])\n\n</code></pre>"},{"location":"nodes/tags/#removetags","title":"RemoveTags","text":"<p>Removes tags from all images in a dataset.</p>"},{"location":"nodes/tags/#parameters_2","title":"Parameters","text":"<ul> <li><code>tags</code>: A list of tags to remove from each image</li> </ul>"},{"location":"nodes/tags/#output-properties_2","title":"Output properties","text":"<ul> <li><code>image.tags</code>: The tags of the image, after removing the specified tags</li> </ul>"},{"location":"nodes/tags/#example_2","title":"Example","text":"<pre><code># Single tag\ndataset &gt;&gt; RemoveTags(\"labrador\")\n\n# Multiple tags\ndataset &gt;&gt; RemoveTags([\"labrador\", \"poodle\", \"labradoodle\"])\n</code></pre>"},{"location":"nodes/tags/#rewritetags","title":"RewriteTags","text":"<p>Rewrites tags for all images in a dataset.</p>"},{"location":"nodes/tags/#parameters_3","title":"Parameters","text":"<ul> <li><code>mapping</code>: A dictionary mapping old tag names to new tag names</li> </ul>"},{"location":"nodes/tags/#output-properties_3","title":"Output properties","text":"<ul> <li><code>image.tags</code>: The tags of the image, after rewriting the tags</li> </ul>"},{"location":"nodes/tags/#example_3","title":"Example","text":"<pre><code>dataset &gt;&gt; RewriteTags({\n    \"outdoors\": \"outside\",\n    \"indoors\": \"inside\"\n})\n</code></pre>"},{"location":"nodes/utils/","title":"Utils","text":""},{"location":"nodes/utils/#apply","title":"Apply","text":"<p>Applies a function to each image in a dataset.</p>"},{"location":"nodes/utils/#parameters","title":"Parameters","text":"<ul> <li><code>fn</code>: A function that takes an image and returns nothing.</li> </ul>"},{"location":"nodes/utils/#example","title":"Example","text":"<pre><code>\ndataset &gt;&gt; Apply(lambda image: image.caption.value = image.caption.value.upper())\n</code></pre>"},{"location":"nodes/utils/#concat","title":"Concat","text":"<p>The concat node is used to concatenate one or more datasets.</p>"},{"location":"nodes/utils/#parameters_1","title":"Parameters","text":"<ul> <li><code>*nodes</code>: One or more nodes.</li> </ul>"},{"location":"nodes/utils/#examples","title":"Examples","text":"<pre><code># Direct usage\nConcat(Load(\"/path/to/photos_of_me\"), Load(\"/path/to/photos_of_dogs\"))\n\n# With &lt;&lt; syntax\nConcat &lt;&lt; Load(\"/path/to/photos_of_me\") &lt;&lt; Load(\"/path/to/photos_of_dogs\")\n</code></pre>"},{"location":"nodes/utils/#fail","title":"Fail","text":"<p>The fail node is used to fail the workflow with an error message. This is useful for debugging and testing.</p>"},{"location":"nodes/utils/#parameters_2","title":"Parameters","text":"<ul> <li><code>message</code> (default: <code>\"error\"</code>): The error message.</li> </ul>"},{"location":"nodes/utils/#example_1","title":"Example","text":"<pre><code>dataset &gt;&gt; Fail(\"Something went wrong\")\n</code></pre>"},{"location":"nodes/utils/#info","title":"Info","text":"<p>The info node prints information about the images in a dataset to stdout. This is useful for debugging small datasets and eyeballing results.</p> <p>For larger datasets, use the <code>index.html</code> file in the output directory next to the images. This contains the same info, but can be viewed in a web browser.</p>"},{"location":"nodes/utils/#parameters_3","title":"Parameters","text":"<ul> <li><code>include_hidden_properties</code> (default: <code>False</code>): If <code>True</code>, hidden properties will be included in the output.</li> </ul>"},{"location":"nodes/utils/#example_2","title":"Example","text":"<pre><code>dataset &gt;&gt; Info\n</code></pre>"},{"location":"nodes/utils/#map","title":"Map","text":"<p>Maps a function over the images in a dataset.</p>"},{"location":"nodes/utils/#parameters_4","title":"Parameters","text":"<ul> <li><code>fn</code>: A function that takes an image and returns a new image.</li> </ul>"},{"location":"nodes/utils/#example_3","title":"Example","text":"<pre><code>dataset &gt;&gt; Map(lambda image: image.with_props(caption=image.caption.value.upper()))\n</code></pre>"},{"location":"nodes/utils/#set","title":"Set","text":"<p>Sets properties on an image.</p>"},{"location":"nodes/utils/#parameters_5","title":"Parameters","text":"<ul> <li><code>**kwargs</code>: The properties to set.</li> </ul>"},{"location":"nodes/utils/#output-properties","title":"Output Properties","text":"<ul> <li><code>image.{key}</code>: Properties specified in <code>kwargs</code></li> </ul>"},{"location":"nodes/utils/#example_4","title":"Example","text":"<pre><code>dataset &gt;&gt; Set(tags=[\"person\", \"ohwx\"], myprop=12345, caption=\"my caption\")\n</code></pre>"},{"location":"nodes/utils/#shuffle","title":"Shuffle","text":"<p>Shuffles the images in the dataset.</p>"},{"location":"nodes/utils/#example_5","title":"Example","text":"<pre><code>Shuffle\n</code></pre>"},{"location":"nodes/utils/#sleep","title":"Sleep","text":"<p>The sleep node is used to pause the workflow for a specified number of seconds. This is useful for testing and debugging.</p>"},{"location":"nodes/utils/#parameters_6","title":"Parameters","text":"<ul> <li><code>seconds</code>: The number of seconds to sleep.</li> </ul>"},{"location":"nodes/utils/#example_6","title":"Example","text":"<pre><code>Sleep(5)\n</code></pre>"},{"location":"nodes/utils/#sorted","title":"Sorted","text":"<p>Sorts images in the dataset based on a <code>key</code> function.</p>"},{"location":"nodes/utils/#parameters_7","title":"Parameters","text":"<ul> <li><code>key</code>: A function that takes an image and returns a value to sort by.</li> <li><code>reverse</code> (default: <code>False</code>): If <code>True</code>, the images will be sorted in descending order.</li> </ul>"},{"location":"nodes/utils/#example_7","title":"Example","text":"<pre><code>dataset &gt;&gt; Sorted(lambda image: image.aesthetic_score.value, reverse=True)\n</code></pre>"},{"location":"nodes/utils/#take","title":"Take","text":"<p>The take node is used to take a fixed number of images from a dataset. </p> <p>It can also be used for random sampling.</p>"},{"location":"nodes/utils/#parameters_8","title":"Parameters","text":"<ul> <li><code>n</code>: The number of images to take.</li> <li><code>random</code> (default: <code>False</code>): If <code>True</code>, images will be taken randomly.</li> <li><code>seed</code> (default: <code>None</code>): The seed for the random number generator, in case you want to select the same random images repeatedly.</li> </ul>"},{"location":"nodes/utils/#example_8","title":"Example","text":"<pre><code># Take the first 10 images\ndataset &gt;&gt; Take(10)\n\n# Take 10 random images\ndataset &gt;&gt; Take(10, random=True)\n\n# Take 10 random images, but always the same ones\ndataset &gt;&gt; Take(10, random=True, seed=42)\n</code></pre>"},{"location":"nodes/watermarkremoval/","title":"Watermark Removal","text":"<p>The EdgeWatermarkRemoval node uses Florence-2 to detect watermarks, logos, and text near image edges and automatically crop them out.</p>"},{"location":"nodes/watermarkremoval/#usage","title":"Usage","text":"<pre><code>from beprepared.nodes import EdgeWatermarkRemoval\n\n# Basic usage with default settings\ndataset &gt;&gt; EdgeWatermarkRemoval() &gt;&gt; Save(\"output\")\n\n# Custom crop threshold\ndataset &gt;&gt; EdgeWatermarkRemoval(max_crop_percent=0.05) &gt;&gt; Save(\"output\")\n\n# Preview mode - draws bounding boxes instead of cropping so you can see what will happen\ndataset &gt;&gt; EdgeWatermarkRemoval(preview_crop=True) &gt;&gt; Save(\"output\")\n</code></pre>"},{"location":"nodes/watermarkremoval/#parameters","title":"Parameters","text":"<ul> <li><code>max_crop_percent</code> (float, default=0.15): Maximum proportion of width/height that can be cropped from any edge. Value between 0.0-1.0.</li> <li><code>preview_crop</code> (bool, default=False): If True, draws bounding boxes and proposed crop region on the original image instead of actually cropping the image.</li> </ul>"},{"location":"nodes/watermarkremoval/#how-it-works","title":"How it Works","text":"<ol> <li>Uses Florence-2's caption-to-phrase-grounding to detect watermarks, logos and text in images</li> <li>Filters detections to only consider those that:</li> <li>Are completely within max_crop_percent (15%) of any edge</li> <li>Come within 3% of an edge (to avoid false positives)</li> <li>For valid watermarks, determines which edge would require the smallest crop to remove all watermarks</li> <li>Crops that edge if a valid solution is found</li> </ol>"},{"location":"nodes/watermarkremoval/#example","title":"Example","text":"<pre><code>(\n    Load(\"input_images\") \n    &gt;&gt; EdgeWatermarkRemoval(max_crop_percent=0.15)\n    &gt;&gt; Save(\"output_images\")\n)\n</code></pre> <p>This will: 1. Load images from \"input_images\" directory 2. Detect and remove watermarks near edges, cropping up to 15% from one edge if needed 3. Save the processed images to \"output_images\" directory</p>"}]}